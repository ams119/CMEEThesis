class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
unique(fddata$Locations)
# Specific dataset
df1 = read.csv("~/Documents/Hefty_Data/dates_weird.csv", header = T, stringsAsFactors = F)
str(df1)
Latitudes = df1$Latitudes
Longitudes = df1$Longitudes
Locations = df1$Locations
points = data.frame(Locations, Longitudes, Latitudes)
# Pull out only distinct locations of trap data
locs <- distinct(points, Locations, Longitudes, Latitudes)
# Load in packages
library(sf)
library(rgdal)
library(maps)
library(sp)
library(readr)
library(tidyverse)
library(ff)
library(ffbase)
library(tmap)
library(tmaptools)
points = data.frame(Locations, Longitudes, Latitudes)
# Pull out only distinct locations of trap data
locs <- distinct(points, Locations, Longitudes, Latitudes)
# Set the crs
project_crs <- "+proj=longlat +WGS84 (EPSG: 4326) +init=epsg:4326 +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"
# Create locs as sf object
locs = st_as_sf(locs, coords = c("Longitudes", "Latitudes"), crs = project_crs, agr = "constant")
rm(points)
# Obtain USA state map from maps package
usa = st_as_sf(maps::map("state", fill = TRUE, plot = FALSE))
usa = st_set_crs(usa, project_crs)
# Obtain Florida from the USA map
state = subset(usa, grepl("california", usa$ID))
usa <- st_buffer(usa, 0)
# Plot locations
tm_shape(usa) +
tm_polygons() +
tm_shape(locs) +
tm_dots()
unique(Locations)
library(stringr)
fddata = read.csv("~/Documents/Hefty_Data/dates_weird.csv", stringsAsFactors = F)
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
# In fddata, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
## I want to check how many are single day intervals
# first extract from and to dates
dates = strsplit(probs$Collection.date.range, split = " TO ")
from = sapply(dates, "[[" , 1)
to = sapply(dates, "[[" , 2)
dates = data.frame(from, to)
uprobs
TOprobs
## I want to check how many are single day intervals
# first extract from and to dates
dates = strsplit(probs$Collection.date.range[TOprobs], split = " TO ")
from = sapply(dates, "[[" , 1)
to = sapply(dates, "[[" , 2)
dates = data.frame(from, to)
# now find the differences between from and to columns
dates$diff = difftime(as.POSIXct(dates$to), as.POSIXct(dates$from), units = "days")
hist(as.numeric(dates$diff))
summary(as.numeric(dates$diff))
?table
freqs = table(dates$diff)
freqs
hist(log(as.numeric(dates$diff)))
freqs_multi = freqs[-1]
freqs
freqs_multi
?round
# now find the differences between from and to columns
dates$diff = round(difftime(as.POSIXct(dates$to), as.POSIXct(dates$from), units = "days"))
hist(as.numeric(dates$diff))
summary(as.numeric(dates$diff))
freqs = table(dates$diff)
freqs
dates = data.frame(dates, probs[TOprobs])
dates = data.frame(dates, probs[TOprobs,])
class(freqs)
# Look at locations where these are happening
unique(dates$Locations)
#Check out the really extreme example: interval of 60 days
sixty = dates[which(dates$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
dates = distinct(dates, Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(dates$diff)
freqs
table(fddata$Collection.protocols)
table(fddata$Sample.type)
table(probs$Collection.protocols)
table(fddata$Attractants)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
library(stringr)
fddata = read.csv("~/Documents/Hefty_Data/dates_weird.csv", stringsAsFactors = F)
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
## I want to check how many are single day intervals
# first extract from and to dates
dates = strsplit(probs$Collection.date.range[TOprobs], split = " TO ")
from = sapply(dates, "[[" , 1)
to = sapply(dates, "[[" , 2)
dates = data.frame(from, to)
# now find the differences between from and to columns
dates$diff = round(difftime(as.POSIXct(dates$to), as.POSIXct(dates$from), units = "days"))
dates = data.frame(dates, probs[TOprobs,])
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
unique(allspecs$Collection.protocols)
table(allspecs$Collection.protocols)
#Check out the really extreme example: interval of 60 days
sixty = dates[which(dates$diff == 60),]
View(sixty)
#Check out the really extreme example: interval of 60 days
sixty = dates[which(allspecs$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected), diff = mean(diff), na.rm = T) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
#Check out the really extreme example: interval of 60 days
sixty = dates[which(allspecs$diff == 60),]
View(sixty)
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
#Check out the really extreme example: interval of 60 days
sixty = dates[which(allspecs$diff == 60),]
View(sixty)
#Check out the really extreme example: interval of 60 days
sixty = allspecs[which(allspecs$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Locations", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Locations, Latitudes, Longitudes, to, from, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
#Check out the really extreme example: interval of 60 days
sixty = allspecs[which(allspecs$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Locations", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Locations, Latitudes, Longitudes, from, to, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
#Check out the really extreme example: interval of 60 days
sixty = allspecs[which(allspecs$diff == 60),]
View(sixty)
table(fddata$Collection.protocols)
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Polk (Iowa (United States))")])
table(fddata$Collection.protocols[which(fddata$Locations == "Woodbury")])
table(fddata$Collection.protocols[which(fddata$Locations == "Riverside")])
fddata = fddata[which(fddata$Locations != "Riverside"),]
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
## I want to check how many are single day intervals
# first extract from and to dates
dates = strsplit(probs$Collection.date.range[TOprobs], split = " TO ")
from = sapply(dates, "[[" , 1)
to = sapply(dates, "[[" , 2)
dates = data.frame(from, to)
# now find the differences between from and to columns
dates$diff = round(difftime(as.POSIXct(dates$to), as.POSIXct(dates$from), units = "days"))
dates = data.frame(dates, probs[TOprobs,])
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Locations", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Locations, Latitudes, Longitudes, from, to, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - nrow(TOprobs) - nrow(commaprobs) - nrow(three_TOprobs) - nrow(three_commaprobs)
nrow(probs) - nrow(TOprobs) - nrow(commaprobs) - nrow(three_TOprobs) #- nrow(three_commaprobs)
nrow(probs)
nrow(TOprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs),]
probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs)]
four_commaprobs = probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs)]
four_commaprobs = three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
remain = probs$Collection.date.range[-c(TOprobs, commaprobs, three_commaprobs, three_TOprobs, four_commaprobs)]
remain
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(four_commaprobs)
table(probs$Collection.protocols[four_commaprobs])
library(stringr)
fddata = read.csv("~/Documents/Hefty_Data/dates_weird.csv", stringsAsFactors = F)
fddata = fddata[which(fddata$Locations != "Riverside"),]
class(fddata$Collection.date.range)
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Polk (Iowa (United States))")])
table(fddata$Collection.protocols[which(fddata$Locations == "Woodbury")])
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[four_commaprobs])
library(stats)
library(MARSS)
library(forecast)
library(datasets)
library(devtools)
devtools::install_github("nwfsc-timeseries/atsalibrary")
data(NHTemp, package = "atsalibrary")
devtools::install_github("nwfsc-timeseries/atsalibrary")
devtools::install_github("nwfsc-timeseries/atsalibrary")
install.packages('rjags')
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
# Define safety function
continue = function() {
answer = readline('Do you wish to continue? (y/n)')
if(substr(answer, 1, 1) == "n"){
cat("Skipping to next location in the loop\n")
return(1)
}
if(substr(answer, 1, 1) == "y"){
cat("You'd better know what you're doing, then!\n")
return(0)
}
else{
cat("Answer given was not an option. Please try again.\n")
continue()
}
}
## Begin loop to aggregate files
files = list.files(pattern = "_TS.csv")
i = 1
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange_monthly.csv_monthly.csv")
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv_monthly.csv")
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _weekly.csv", header = T, stringsAsFactors = F)
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
d$date_m
as.numeric(format(as.Date(d$date_m[1], format="%Y-%U"),"%U"))
d$date_m[1]
as.Date(d$date_m[1], format="%Y-%U"),"%U")
as.Date(d$date_m[1], format="%Y-%U")
as.Date(d$date_m[1], format="%Y-%m"),"%m")
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
as.numeric(format(as.Date(d$date_m[1], format="%Y-%m"),"%m"))
d$date_m[1]
format(as.Date(d$date_m[1], format="%Y-%m"),"%m")
as.Date(d$date_m[1], format="%Y-%m")
class(d$date_m[1])
as.Date(d$date_m[1], format="%Y-%m")
as.Date(d$date_m)
as.Date(d$date_m, format = "%Y-%m")
d$date_dm
as.Date(d$date_dm, format = "%Y-%m-%d")
as.numeric(format(as.Date(d$date_dm[1], format="%Y-%m-%d"),"%m"))
d$Year[1]
# and the end:
end = c(as.numeric(format(as.Date(d$date_dm[nrow(d)], format="%Y-%m-%d"),"%m")), d$Year[nrow(d)])
# Create ts object from the chosen species data
# Find the start of this dataset: c(month number of start, year of start)
start = c(as.numeric(format(as.Date(d$date_dm[1], format="%Y-%m-%d"),"%m")), d$Year[1])
# Create ts object from the chosen species data
# Find the start of this dataset: c(month number of start, year of start)
start = c(d$Year[1], as.numeric(format(as.Date(d$date_dm[1], format="%Y-%m-%d"),"%m")))
# and the end:
#end = c(as.numeric(format(as.Date(d$date_dm[nrow(d)], format="%Y-%m-%d"),"%m")), d$Year[nrow(d)])
moz = ts(data = d$Culex.erraticus, frequency = 12, start = start)
# Now plot
plot.ts(moz)
# also make temp time series
temp = ts(data = d$temp_mean, frequency = 12, start = start)
# Merge our ts:
both = ts.union(moz, temp)
# Plot
plot(both, main = "Temp and abundance")
# Plot
plot(both, main = "Temp and abundance", yax.flip = T)
# for even numbered values of k, use only 1/2 weight for 2 most extreme values
ma_weights = c(1/2, rep(1, 11), 1/2)/12
?filter
# Estimate the trend (m):
moz_trend = filter(moz, filter = ma_weights, method = "convo", sides = 2)
# Plot the trend
plot.ts(moz_trend)
## Find seasonal effects by subtraction
moz_seas = moz - moz_trend
plot.ts(moz_seas, ylab = 'seasonal effect + errors', xlab = 'month')
### Obtain average seasonal effect (per year): ###
# length of ts
ll = length(moz_seas)
## frequency (12 months)
ff = frequency(moz_seas)
## number of periods:
periods = ll%/%ff
# index of cumulative month
index = seq(1, 11, by = ff) - 1
?numeric
# get mean by month
mm = numeric(ff)
mm[i] = mean(moz_seas[index + i], na.rm = T)
for(i in 1:ff){
mm[i] = mean(moz_seas[index + i], na.rm = T)
}
### Obtain average seasonal effect (per year): ###
# length of ts
ll = length(moz_seas)
## frequency (12 months)
ff = frequency(moz_seas)
## number of periods:
periods = ll%/%ff
# index of cumulative month
index = seq(1, 11, by = ff) - 1
# get mean by month
mm = numeric(ff)
for(i in 1:ff){
mm[i] = mean(moz_seas[index + i], na.rm = T)
}
library(stats)
library(MARSS)
library(forecast)
library(datasets)
library(devtools)
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
# Create ts object from the chosen species data
# Find the start of this dataset: c(month number of start, year of start)
start = c(d$Year[1], as.numeric(format(as.Date(d$date_dm[1], format="%Y-%m-%d"),"%m")))
# and the end:
#end = c(as.numeric(format(as.Date(d$date_dm[nrow(d)], format="%Y-%m-%d"),"%m")), d$Year[nrow(d)])
moz = ts(data = d$Culex.erraticus, frequency = 12, start = start)
# Now plot
plot.ts(moz) # violates stationarity due to 1. seasonality and 2. possibly increasing? could also be change in variance
# also make temp time series
temp = ts(data = d$temp_mean, frequency = 12, start = start)
# Merge our ts:
both = ts.union(moz, temp)
# Plot
plot(both, main = "Temp and abundance", yax.flip = T)
# for even numbered values of k, use only 1/2 weight for 2 most extreme values
ma_weights = c(1/2, rep(1, 11), 1/2)/12
# Estimate the trend (m):
moz_trend = filter(moz, filter = ma_weights, method = "convo", sides = 2)
# Plot the trend
plot.ts(moz_trend)
moz_trend
## Find seasonal effects + random error by subtraction
moz_seas = moz - moz_trend
plot.ts(moz_seas, ylab = 'seasonal effect + errors', xlab = 'month')
### Obtain average seasonal effect (per year): ###
# length of ts
ll = length(moz_seas)
## frequency (12 months)
ff = frequency(moz_seas)
## number of periods:
periods = ll%/%ff
# index of cumulative month
index = seq(1, 11, by = ff) - 1
# get mean by month
mm = numeric(ff)
for(i in 1:ff){
mm[i] = mean(moz_seas[index + i], na.rm = T)
}
mm
moz_seas
i = 1
index + i
# index of cumulative month
index = seq(1, 11, by = ff) - 1
index
?seq
