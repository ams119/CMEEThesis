summarise(precip_days= sum(precip_days, na.rm = T), obs = sum(obs, na.rm = T)) %>%
left_join(monthly, by = c("date_m")) %>%
arrange(years, date_m)
View(monthly)
# Aggregate counts to week and join
monthly =
daily %>%
group_by(date_m) %>%
dplyr::select("date_m", "precip_days", "obs") %>%
summarise(precip_days= sum(precip_days, na.rm = T), obs = sum(obs, na.rm = T)) %>%
left_join(monthly, by = c("date_m")) %>%
# Find the coefficient of variation of temp and precip
monthly$precip_cv = monthly$precip_sd/monthly$precip_mean
monthly$temp_cv = monthly$temp_sd/monthly$temp_mean
# Set abun of no count weeks to NA
monthly$Specimens.collected = replace(monthly$Specimens.collected, monthly$obs == 0, NA)
# Convert character date column to date format. Must arbitrarily add day of the month to make
# date formatting work
monthly$date_dm = paste0(monthly$date_m, "-01")
monthly$date_dm = as.Date(monthly$date_dm, format = "%Y-%m-%d")
plot_m = ggplot(monthly, aes(x=date_dm, y = `Specimens.collected`/1000000)) +
geom_line(col = "purple") + xlab("") +
scale_x_date(date_labels = "%Y") +
ggtitle(paste0(locale, " Monthly")) +
theme_bw() + labs(y = "Mosquitoes Collected (millions)",
x = "Time")
plot_m = ggplot(monthly, aes(x=date_dm, y = `Specimens.collected`/1000)) +
geom_line(col = "purple") + xlab("") +
scale_x_date(date_labels = "%Y") +
ggtitle(paste0(locale, " Monthly")) +
theme_bw() + labs(y = "Mosquitoes Collected (thousands)",
x = "Time")
plot_m
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files()
i = 1
setwd("~/Documents/Hefty_Data/Extracted_Data/")
dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
files[i]
?list.files
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(include.dirs = F)
files = list.files(include.dirs = F)
files = list.files(include.dirs = FALSE)
files = list.files(recursive = F, include.dirs = FALSE)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(recursive = F, include.dirs = FALSE)
files = list.files(recursive = T, include.dirs = FALSE)
i = 1
setwd("~/Documents/Hefty_Data/Extracted_Data/")
dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
setwd(paste0("~/Documents/Hefty_Data/Extracted_Data/",sub(".csv", "", files[i])))
setwd("~/Documents/Hefty_Data/Extracted_Data/")
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(recursive = T, include.dirs = FALSE)
i = 1
setwd("~/Documents/Hefty_Data/Extracted_Data/")
dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd(paste0("~/Documents/Hefty_Data/Extracted_Data/",sub(".csv", "", files[i])))
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_weekly.pdf"), width = 5, height = 4)
plot_w
dev.off()
pdf(file = paste0(locale, "_biweekly.pdf"), width = 5, height = 4)
plot_bw
dev.off()
pdf(file = paste0(locale, "_monthly.pdf"), width = 5, height = 4)
plot_m
dev.off()
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
par(mfrow = c(3, 1))
plot_w
plot_bw
plot_m
dev.off()
par(mfrow = c(3, 1))
plot_w
plot_bw
plot_m
?par
par(mfrow = c(2, 2))
plot_w
plot_bw
plot_m
library(gridExtra)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
dev.off()
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(recursive = T, include.dirs = FALSE)
i = 1
#setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
source("~/Documents/CMEEThesis/Code/aggregation.R")
#setwd(paste0("~/Documents/Hefty_Data/Extracted_Data/",sub(".csv", "", files[i])))
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
dev.off()
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(recursive = T, include.dirs = FALSE)
i = 1
#setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(recursive = T, include.dirs = FALSE)
i = 1
#setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(recursive = T, include.dirs = FALSE)
i = 1
setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/Aggregated")
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
dev.off()
View(biweekly)
class(biweekly$years)
?seq
brks = seq(0, 26 * length(yrs), by = 26)
brks
brks = seq(0, 26 * length(yrs), by = 26.5)
brks = seq(0, 26 * (length(yrs)-1), by = 26.5)
brks
brks = seq(0, 26 * length(yrs), by = 26.5)
as.numeric(yrs[length(yrs)]) + 1
as.character(as.numeric(yrs[length(yrs)]) + 1)
next_year = as.character(as.numeric(yrs[length(yrs)]) + 1)
plot_bw = ggplot(biweekly, aes(x=ids, y = `Specimens.collected`/1000)) +
geom_line(col = "blue") + xlab("") +
scale_x_continuous(Time, breaks = brks, labels = c(yrs, next_year))
brks
length(yrs)
length(yrs)
brks = seq(0, 26 * (length(yrs) + 1), by = 26.5)
brks
next_year = as.character(as.numeric(yrs[length(yrs)]) + 1)
plot_bw = ggplot(biweekly, aes(x=ids, y = `Specimens.collected`/1000)) +
geom_line(col = "blue") + xlab("") +
scale_x_continuous(Time, breaks = brks, labels = c(yrs, next_year))
plot_bw = ggplot(biweekly, aes(x=ids, y = `Specimens.collected`/1000)) +
geom_line(col = "blue") + xlab("") +
scale_x_continuous("Time", breaks = brks, labels = c(yrs, next_year))
plot_bw = ggplot(biweekly, aes(x=ids, y = `Specimens.collected`/1000)) +
geom_line(col = "blue") + xlab("") +
scale_x_continuous("Time", breaks = brks, labels = c(yrs, next_year)) +
ggtitle(paste0(locale, " Biweekly")) +
theme_bw() + labs(y = "Mosquitoes Collected (thousands)")
plot_bw
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(recursive = T, include.dirs = FALSE)
i = 1
setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(pattern = "_TS.csv")
i = 1
setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/Aggregated")
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
dev.off()
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(pattern = "_TS.csv")
for(i in 1:length(files)){
setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/Aggregated")
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
dev.off()
}
brks
yrs
c(yrs, next_year)
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(pattern = "_TS.csv")
for(i in 1:length(files)){
setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/Aggregated")
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
dev.off()
}
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(pattern = "_TS.csv")
for(i in 1:length(files)){
setwd("~/Documents/Hefty_Data/Extracted_Data/")
#dir.create(sub(".csv", "", files[i]))
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/Aggregated")
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
dev.off()
rm(list=setdiff(ls(), c("i", "files")))
}
cat(paste("Now aggregating data for", sub(".csv", "", files[i])))
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(pattern = "_TS.csv")
i = 1
cat(paste("Now aggregating data for", sub(".csv", "", files[i])))
i = 4
cat(paste("Now aggregating data for", sub(".csv", "", files[i])))
i = 5
cat(paste("Now aggregating data for", sub(".csv", "", files[i])))
setwd("~/Documents/Hefty_Data/Extracted_Data/")
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
#mapped = read.csv("~/Documents/Hefty_Data/Extracted_Data/Manatee_clim_TS.csv", header = T, stringsAsFactors = F)
locale = as.character(na.omit(mapped$Locations)[1])
locale = sub(" ", "", locale)
#mapped = read.csv("~/Documents/Hefty_Data/Extracted_Data/Manatee_clim_TS.csv", header = T, stringsAsFactors = F)
locale = as.character(na.omit(mapped$Locations)[1])
locale = sub(" *", "", locale)
#mapped = read.csv("~/Documents/Hefty_Data/Extracted_Data/Manatee_clim_TS.csv", header = T, stringsAsFactors = F)
locale = as.character(na.omit(mapped$Locations)[1])
locale = sub("\s*", "", locale)
locale = sub("\\s*", "", locale)
#mapped = read.csv("~/Documents/Hefty_Data/Extracted_Data/Manatee_clim_TS.csv", header = T, stringsAsFactors = F)
locale = as.character(na.omit(mapped$Locations)[1])
locale = sub("\\s.*", "", locale)
library(gridExtra)
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(pattern = "_TS.csv")
pdf(file = paste0(locale, "_aggregations.pdf"), width = 8.5, height = 11)
pdf(file = "all_aggregations.pdf", width = 8.5, height = 11)
for(i in 1:length(files)){
cat(paste("Now aggregating data for", sub(".csv", "", files[i])))
setwd("~/Documents/Hefty_Data/Extracted_Data/")
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/Aggregated")
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
rm(list=setdiff(ls(), c("i", "files")))
}
dev.off()
library(gridExtra)
setwd("~/Documents/Hefty_Data/Extracted_Data/")
files = list.files(pattern = "_TS.csv")
pdf(file = "/Aggregated/all_aggregations.pdf", width = 8.5, height = 11)
pdf(file = "all_aggregations.pdf", width = 8.5, height = 11)
dev.off()
pdf(file = "Aggregated/all_aggregations.pdf", width = 8.5, height = 11)
dev.off()
pdf(file = "Aggregated/all_aggregations.pdf", width = 8.5, height = 11)
for(i in 1:length(files)){
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
setwd("~/Documents/Hefty_Data/Extracted_Data/")
mapped = read.csv(files[i], header = T, stringsAsFactors = F)
source("~/Documents/CMEEThesis/Code/aggregation.R")
setwd("~/Documents/Hefty_Data/Extracted_Data/Aggregated")
write.csv(weekly, paste0(locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0(locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0(locale, "_monthly.csv"), row.names = F)
grid.arrange(plot_w, plot_bw, plot_m, nrow = 3)
rm(list=setdiff(ls(), c("i", "files")))
}
dev.off()
# Load in packages
library(sf)
library(rgdal)
library(maps)
library(sp)
library(readr)
library(tidyverse)
library(ff)
library(ffbase)
library(tmap)
library(tmaptools)
# Load in a sample of the vectdyn data
sample = read.csv("../../Hefty_Data/vectdyn_fulldata.csv", header = T, nrows = 500)
# Find the classes of each column
sampleclasses = sapply(sample, class)
# Latitude and Longitude are both numeric- switch to factor to better deal with data type errors
classesswitch = sampleclasses
classesswitch[10:11] = "factor"
# Read in full data frame as ff data frame with column classes specified
# Time because it's fun
print("Reading in abundance data as an ff data frame.")
start = Sys.time()
df1 = read.csv.ffdf(file = "../../Hefty_Data/vectdyn_fulldata.csv", sep = ",", colClasses = classesswitch)
end = Sys.time()
end-start
# Load in a sample of the vectdyn data
sample = read.csv("../../Hefty_Data/vectdyn_fulldata.csv", header = T, nrows = 500)
# Load in a sample of the vectdyn data
sample = read.csv("~/Documents/Hefty_Data/vectdyn_fulldata.csv", header = T, nrows = 500)
# Find the classes of each column
sampleclasses = sapply(sample, class)
# Latitude and Longitude are both numeric- switch to factor to better deal with data type errors
classesswitch = sampleclasses
classesswitch[10:11] = "factor"
# Read in full data frame as ff data frame with column classes specified
# Time because it's fun
print("Reading in abundance data as an ff data frame.")
start = Sys.time()
df1 = read.csv.ffdf(file = "~/Documents/Hefty_Data/vectdyn_fulldata.csv", sep = ",", colClasses = classesswitch)
end = Sys.time()
end-start
# Pull out location-related columns
Latitudes = as.numeric(levels(df1$Latitudes)[df1$Latitudes[]])
Longitudes = as.numeric(levels(df1$Longitudes)[df1$Longitudes[]])
Locations = as.character(levels(df1$Locations)[df1$Locations[]])
points = data.frame(Locations, Longitudes, Latitudes)
# Pull out only distinct locations of trap data
locs <- distinct(points, Locations, Longitudes, Latitudes)
# Set the crs
project_crs <- "+proj=longlat +WGS84 (EPSG: 4326) +init=epsg:4326 +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"
# Create locs as sf object
locs = st_as_sf(locs, coords = c("Longitudes", "Latitudes"), crs = project_crs, agr = "constant")
rm(points)
# Obtain USA state map from maps package
usa = st_as_sf(maps::map("state", fill = TRUE, plot = FALSE))
usa = st_set_crs(usa, project_crs)
# Plot locations
tm_shape(usa) +
tm_polygons() +
tm_shape(locs) +
tm_dots()
locs_nd = locs[state,]
# Obtain Florida from the USA map
state = subset(usa, grepl("north dakota", usa$ID))
usa <- st_buffer(usa, 0)
locs_nd = locs[state,]
sites = unique(locs_nd$Locations)
sites
nd_data = df1[df1$Locations %in% sites,]
cass = nd_data[nd_data$Locations == "Cass (North Dakota)",]
cass = data.frame(cass)
colnames(cass)
unique(cass$Collection.date.range)
# Obtain Florida from the USA map
state = subset(usa, grepl("california", usa$ID))
usa <- st_buffer(usa, 0)
# Let's say we just want the points from Florida:
locs_fd = locs[state,]
tm_shape(usa) +
tm_polygons() +
tm_shape(locs_fd) +
tm_dots()
# Now find which study sites are relevant to this subset
sites = unique(locs_fd$Locations)
sites
# Pull data points matching these sites from ffdf
fd_data = df1[df1$Locations %in% sites,]
fd_data = data.frame(fd_data)
fd_data$Collection.date.range[1]
# Obtain Florida from the USA map
state = subset(usa, grepl("north dakota", usa$ID))
usa <- st_buffer(usa, 0)
# Plot locations
tm_shape(usa) +
tm_polygons() +
tm_shape(locs) +
tm_dots()
tm_shape(state) +
tm_polygons() +
tm_shape(locs) +
tm_dots()
# Let's say we just want the points from Florida:
locs_fd = locs[state,]
tm_shape(usa) +
tm_polygons() +
tm_shape(locs_fd) +
tm_dots()
# Now find which study sites are relevant to this subset
sites = unique(locs_fd$Locations)
# Pull data points matching these sites from ffdf
fd_data = df1[df1$Locations %in% sites,]
# Save as csv
print("Saving subsetted data as csv in ../Data/")
write.csv(fd_data, "~/Documents/Hefty_Data/ndcounts.csv", row.names = F)
fddata = read.csv("~/Documents/Hefty_Data/mosquitoAbun/ndcounts.csv", stringsAsFactors = F)
class(fddata$Collection.date.range)
# Obtain Florida from the USA map
state = subset(usa, grepl("california", usa$ID))
usa <- st_buffer(usa, 0)
# Plot locations
tm_shape(usa) +
tm_polygons() +
tm_shape(locs) +
tm_dots()
tm_shape(state) +
tm_polygons() +
tm_shape(locs) +
tm_dots()
# Let's say we just want the points from Florida:
locs_fd = locs[state,]
tm_shape(usa) +
tm_polygons() +
tm_shape(locs_fd) +
tm_dots()
# Now find which study sites are relevant to this subset
sites = unique(locs_fd$Locations)
# Pull data points matching these sites from ffdf
fd_data = df1[df1$Locations %in% sites,]
# Now find which study sites are relevant to this subset
sites = unique(locs_fd$Locations)
sites
sites = sites[-2]
sites
# Now find which study sites are relevant to this subset
sites = unique(locs_fd$Locations)
sites
sites = sites[-3]
sites
# Pull data points matching these sites from ffdf
fd_data = df1[df1$Locations %in% sites,]
write.csv(fd_data, "~/Documents/Hefty_Data/cacounts.csv", row.names = F)
fddata = read.csv("~/Documents/Hefty_Data/mosquitoAbun/ndcounts.csv", stringsAsFactors = F)
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# Just take the first date
fixed = gsub("\\s.*", "", fddata$Collection.date.range)
# Check that every date is accounted for
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fixed)
# every date now matches the pattern, so sub in this column and save
fddata$Collection.date.range = fixed
write.csv(fddata, "~/Documents/Hefty_Data/mosquitoAbun/ndcounts2.csv")
fddata = read.csv("~/Documents/Hefty_Data/mosquitoAbun/cacounts.csv", stringsAsFactors = F)
fddata = read.csv("~/Documents/Hefty_Data/mosquitoAbun/cacounts.csv", stringsAsFactors = F)
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs = unique(probs$Collection.date.range)
# In fddata, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#check
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
# Just take the first date
fixed = gsub("\\s.*", "", fddata$Collection.date.range)
# Check that every date is accounted for
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fixed)
# every date now matches the pattern, so sub in this column and save
fddata$Collection.date.range = fixed
write.csv(fddata, "~/Documents/Hefty_Data/mosquitoAbun/cacounts2.csv")
# Read in abundace dataset
fddata = read.csv("~/Documents/Hefty_Data/mosquitoAbun/cacounts2.csv", header = T, stringsAsFactors = F)
# Create vector of locations
counties = unique(fddata$Locations)
# Extract climate for each location and save time series as csv
for(p in 1:length(counties)){
vector_abun = fddata[fddata$Locations == counties[p],]
source("~/Documents/CMEEThesis/Code/climate_extraction_batch.R")
filename = paste("~/Documents/Hefty_Data/Extracted_Data/", locale,
"_clim_TS.csv", sep = "")
write.csv(vector_abun_clim_time_series, file = filename, row.names = F)
rm(list=setdiff(ls(), c("p", "counties", "fddata")))
}
