unique(dates$Locations)
#Check out the really extreme example: interval of 60 days
sixty = dates[which(dates$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
dates = distinct(dates, Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(dates$diff)
freqs
table(fddata$Collection.protocols)
table(fddata$Sample.type)
table(probs$Collection.protocols)
table(fddata$Attractants)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
library(stringr)
fddata = read.csv("~/Documents/Hefty_Data/dates_weird.csv", stringsAsFactors = F)
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
## I want to check how many are single day intervals
# first extract from and to dates
dates = strsplit(probs$Collection.date.range[TOprobs], split = " TO ")
from = sapply(dates, "[[" , 1)
to = sapply(dates, "[[" , 2)
dates = data.frame(from, to)
# now find the differences between from and to columns
dates$diff = round(difftime(as.POSIXct(dates$to), as.POSIXct(dates$from), units = "days"))
dates = data.frame(dates, probs[TOprobs,])
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
unique(allspecs$Collection.protocols)
table(allspecs$Collection.protocols)
#Check out the really extreme example: interval of 60 days
sixty = dates[which(dates$diff == 60),]
View(sixty)
#Check out the really extreme example: interval of 60 days
sixty = dates[which(allspecs$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected), diff = mean(diff), na.rm = T) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
#Check out the really extreme example: interval of 60 days
sixty = dates[which(allspecs$diff == 60),]
View(sixty)
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Latitudes, Longitudes, to, from, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
#Check out the really extreme example: interval of 60 days
sixty = dates[which(allspecs$diff == 60),]
View(sixty)
#Check out the really extreme example: interval of 60 days
sixty = allspecs[which(allspecs$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Locations", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Locations, Latitudes, Longitudes, to, from, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
#Check out the really extreme example: interval of 60 days
sixty = allspecs[which(allspecs$diff == 60),]
View(sixty)
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Locations", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Locations, Latitudes, Longitudes, from, to, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
#Check out the really extreme example: interval of 60 days
sixty = allspecs[which(allspecs$diff == 60),]
View(sixty)
table(fddata$Collection.protocols)
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Polk (Iowa (United States))")])
table(fddata$Collection.protocols[which(fddata$Locations == "Woodbury")])
table(fddata$Collection.protocols[which(fddata$Locations == "Riverside")])
fddata = fddata[which(fddata$Locations != "Riverside"),]
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
## I want to check how many are single day intervals
# first extract from and to dates
dates = strsplit(probs$Collection.date.range[TOprobs], split = " TO ")
from = sapply(dates, "[[" , 1)
to = sapply(dates, "[[" , 2)
dates = data.frame(from, to)
# now find the differences between from and to columns
dates$diff = round(difftime(as.POSIXct(dates$to), as.POSIXct(dates$from), units = "days"))
dates = data.frame(dates, probs[TOprobs,])
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Locations", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Locations, Latitudes, Longitudes, from, to, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - nrow(TOprobs) - nrow(commaprobs) - nrow(three_TOprobs) - nrow(three_commaprobs)
nrow(probs) - nrow(TOprobs) - nrow(commaprobs) - nrow(three_TOprobs) #- nrow(three_commaprobs)
nrow(probs)
nrow(TOprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs),]
probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs)]
four_commaprobs = probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs)]
four_commaprobs = three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
remain = probs$Collection.date.range[-c(TOprobs, commaprobs, three_commaprobs, three_TOprobs, four_commaprobs)]
remain
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(four_commaprobs)
table(probs$Collection.protocols[four_commaprobs])
library(stringr)
fddata = read.csv("~/Documents/Hefty_Data/dates_weird.csv", stringsAsFactors = F)
fddata = fddata[which(fddata$Locations != "Riverside"),]
class(fddata$Collection.date.range)
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Polk (Iowa (United States))")])
table(fddata$Collection.protocols[which(fddata$Locations == "Woodbury")])
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[four_commaprobs])
library(stats)
library(MARSS)
library(forecast)
library(datasets)
library(devtools)
library(tseries)
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
# Create ts object from the chosen species data: classical decomposition model
# Find the start of this dataset: c(month number of start, year of start)
start = c(d$Year[1], as.numeric(format(as.Date(d$date_dm[1], format="%Y-%m-%d"),"%m")))
# and the end:
#end = c(as.numeric(format(as.Date(d$date_dm[nrow(d)], format="%Y-%m-%d"),"%m")), d$Year[nrow(d)])
moz = ts(data = d$Culex.erraticus, frequency = 12, start = start)
# Now plot
plot.ts(moz) # violates stationarity due to 1. seasonality and 2. possibly increasing? could also be change in variance
# also make temp time series
temp = ts(data = d$temp_mean, frequency = 12, start = start)
# Merge our ts:
both = ts.union(moz, temp)
# Plot
plot(both, main = "Temp and abundance", yax.flip = T)
# for even numbered values of k, use only 1/2 weight for 2 most extreme values
ma_weights = c(1/2, rep(1, 11), 1/2)/12
# Estimate the trend (m):
moz_trend = filter(moz, filter = ma_weights, method = "convo", sides = 2)
# Plot the trend
plot.ts(moz_trend)
## Find seasonal effects + random error by subtraction
moz_seas = moz - moz_trend
plot.ts(moz_seas, ylab = 'seasonal effect + errors', xlab = 'month')
### Obtain average seasonal effect (per year): ###
# length of ts
ll = length(moz_seas)
## frequency (12 months)
ff = frequency(moz_seas)
## number of periods:
periods = ll%/%ff
# index of cumulative month
index = seq(1, ll, by = ff) - 1
# get mean by month
mm = numeric(ff)
for(i in 1:ff){
mm[i] = mean(moz_seas[index + i], na.rm = T)
}
mm
# Plot monthly seasonal effect
plot.ts(mm, ylab = 'Monthly Seasonal Effect', xlab = 'Month')
# Create time series of seasonal effects:
moz_ts_seas = ts(rep(mm, periods + 1)[seq(ll)], start = start(moz_seas), frequency = ff)
plot(moz_ts_seas)
# Complete the model by obtaining the random errors via subtraction
err = moz - moz_trend - moz_ts_seas
# Now we have 3 components: trend, seasonality, error: plot with data:
plot(cbind(moz, moz_trend, moz_ts_seas, err), main = "", yax.flip = T)
########## Use decompose() to do all of this all at once! WOW! ##################
moz_decomp = decompose(moz)
# figure out how many times you should difference:
ndiff(moz)
# figure out how many times you should difference:
ndiffs(moz)
##### Assess AutoCorrelation#######
## Use ACF and PACF to find q and p, respecitively for AR-I-MA model
acf(moz, lag.max = 36, na.action = na.pass) # na.pass deals with na values by interpolated them
# PACF- finds correlation of residuals with next lag value
# Sharp decrease in PACF means lags close to 0 do good job of predicting present
pacf(moz, lag.max = 36, na.action = na.pass)
######### Cross Correlation Between Time Series ##############
# plot data of interest
plot(cbind(temp, moz), yax.flip = T)
# Find cross correlation:
ccf(temp, moz, ylab = "Cross Correlation", na.action = na.pass)
# I want to see this x axis converted to month:
cc = ccf(temp, moz, ylab = "Cross Correlation", na.action = na.pass)
cc.x = cc$lag
cc.x.months = cc$lag*12
cc.y = cc$acf
nn = cc$n.used
plot(cc.y~cc.x.months, type = 'h', xlab = 'lag', ylab = 'ACF')
abline(h = -1/nn + c(-2, 2)/sqrt(nn), lty = "dashed", col = "blue")
abline(h = 0)
fit
### Use auto.arima() for seasonal ts
# subtract 1 year off moz data for testing forecastin
trainmoz = window(moz, start, c(2016, 12))
par(mfrow = c(3,1))
ndiffs(trainmoz)
plot(diff(trainmoz))
acf(diff(trainmoz))
pacf(diff(trainmoz))
testmoz = window(moz, c(2017, 1), c(2017, 12))
fit = auto.arima(trainmoz)
fit
dev.off()
plot(forecast(fit, h = 12))
points(testmoz)
fit$model
fit$arma
fit
ndiffs(temp)
fit2 = auto.arima(temp)
fit2
fit2$arma
fit3 = auto.arima(d$precip_days)
fit3
fit3$arma
fit5 = auto.arima(d$Specimens.collected)
fit5
fit5 = auto.arima(d$Aedes.aegypti)
fit5
fit5$arma
fit5 = auto.arima(d$Aedes.atlanticus)
fit5
fit5 = auto.arima(d$precip_mean)
fit5
fit5 = auto.arima(d$Culex.erraticus)
fit5
fit5 = auto.arima(ts(d$Culex.erraticus))
fit5
?ts
fit5 = auto.arima(ts(d$Culex.erraticus, frequency = 12))
fit5
fit5 = auto.arima(moz)
fit5
fit5$arma
auto.arima(ts(d$precip_mean, frequency = 12))
fit6 = auto.arima(d$precip_mean)
fit6
fit6$arma
# extract seasonal sarima model: format of auto.arima()$arma = p, q, P, Q, m, d, D
non_seas_comp = fit$arma[c(1, 2, 6)]
fit
seas_comp = fit$arma[c(3,4,7)]
freq = fit$arma[5]
plot(log(moz))
plot(log(d$Culex.erraticus))
plot(log(d$Culex.erraticus + 1))
density(log(d$Culex.erraticus + 1))
density(log(d$Culex.erraticus + 1), na.rm = T)
plot(density(log(d$Culex.erraticus + 1), na.rm = T))
plot(forecast(fit, h = 12))
plot(density(d$Culex.erraticus))
plot(density(d$Culex.erraticus), na.rm = T)
plot(density(d$Culex.erraticus, na.rm = T))
hist(d$Culex.erraticus)
hist(d$Culex.erraticus, breaks = 20)
hist(log(d$Culex.erraticus +1), breaks = 20)
hist(log(d$Culex.erraticus), breaks = 20)
log(0)
plot(ts(log(d$Culex.erraticus)))
plot(diff(ts(log(d$Culex.erraticus))))
fit = auto.arima(ts(log(d$Culex.erraticus)), frequency = 12)
ccf(temp, ts(log(d$Culex.erraticus)))
ccf(temp, ts(log(d$Culex.erraticus), frequency = 12))
?ts
test = ts(d$Culex.erraticus, start = start, frequency = 12)
test = ts(log(d$Culex.erraticus), start = start, frequency = 12)
plot.ts(test)
decompose(test)
plot(decompose(test))
?decompose
decompose(diff(test))
ccf(temp, test)
ccf(temp, test, na.action = na.pass)
plot(test)
plot(ts(log(d$Culex.erraticus + 1)))
plot(ts(d$Culex.erraticus))
hist(d$Culex.erraticus)
hist(d$Culex.erraticus, breaks = 20)
hist(log(d$Culex.erraticus), breaks = 20)
hist(log(d$Culex.erraticus + 1), breaks = 20)
plot(log(d$Culex.erraticus~d$temp_mean))
plot(log(d$Culex.erraticus)~d$temp_mean)
log(0)
log(0.000001)
exp(2.5)
length(d$erraticus == 0)/nrow(!is.na(d$erraticus))
length(d$erraticus == 0)
length(d$Culex.erraticus == 0)
length(d$Culex.erraticus == 0)/nrow(!is.na(d$Culex.erraticus))
length(d$Culex.erraticus == 0)/nrow(!is.na(d$Culex.erraticus))
nrow(!is.na(d$Culex.erraticus))
length(!is.na(d$Culex.erraticus))
!is.na(d$Culex.erraticus)
sum(!is.na(d$Culex.erraticus))
sum(d$Culex.erraticus == 0)
View(d$Culex.erraticus[which(d$Culex.erraticus == 0)])
?decompose
boxplot(d$Culex.erraticus)
?boxplot
boxplot(d$Culex.erraticus~d$Year)
points(testmoz)
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
moz1 = d$Culex.erraticus + 1
templ1 = d$temp_mean[1:(nrow(d)-1)]
test = c(1, 2, 3, 4, 5)
test[2:]
test[-1]
library(mgcv)
# Try simple GAM
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
# Create vector of abundance + 1
moz1 = d$Culex.erraticus + 1
templ1 = d$temp_mean[1:(nrow(d)-1)]
precipl1 = d$precip_days[1:(nrow(d)-1)]
mozl1 = moz1[1:(nrow(d)-1)]
fit = gam(moz1[-1]~ s(templ1) + s(precipl1) + s(log(mozl1)))
fit
summary(fit)
fit$aic
fit2 = gam(moz1[-1]~ s(templ1) + s(precipl1))
summary(fit2)
fit2$aic
fit = gam(moz1[-1]~ s(templ1) + s(precipl1) + s(log(mozl1)), family = Gamma(link = 'log'))
summary(fit)
fit$aic
fit2 = gam(moz1[-1]~ s(templ1) + s(precipl1), family = Gamma(link = 'log'))
summary(fit2)
fit2$aic
choose(3,2)
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F)
barplot(d$Best_TempLag)
barplot(table(d$Best_TempLag))
barplot(table(d$Best_TempLag), labels(cex = 0.8))
barplot(table(d$Best_TempLag), cex.names = 0.8)
barplot(table(d$Best_TempLag), cex.names = 0.5)
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.9)
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
len(d$Best_TempLag[40])
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_TempLag)
barplot(table(d$Best_PrecipMeanLag), cex.names = 0.7)
View(d)
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_TempLag)
barplot(table(d$Best_PrecipMeanLag), cex.names = 0.7)
table(d$Best_TempLag)
d = read.csv('~/Documents/CMEEThesis/Results/GLM_output_weekly.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
setwd('~/Documents/Masters_Thesis/Code/')
d = read.csv('~/Documents/Masters_Thesis/Results/GLM_output_weekly.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_TempLag)
table(d$Best_PrecipDaysLag)
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
barplot(table(d$Best_PrecipMeanLag), cex.names = 0.7)
d = read.csv('~/Documents/Masters_Thesis/Results/GAM_output_weekly.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_PrecipDaysLag)
