sixty = allspecs[which(allspecs$diff == 60),]
View(sixty)
table(fddata$Collection.protocols)
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Polk (Iowa (United States))")])
table(fddata$Collection.protocols[which(fddata$Locations == "Woodbury")])
table(fddata$Collection.protocols[which(fddata$Locations == "Riverside")])
fddata = fddata[which(fddata$Locations != "Riverside"),]
class(fddata$Collection.date.range)
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
## I want to check how many are single day intervals
# first extract from and to dates
dates = strsplit(probs$Collection.date.range[TOprobs], split = " TO ")
from = sapply(dates, "[[" , 1)
to = sapply(dates, "[[" , 2)
dates = data.frame(from, to)
# now find the differences between from and to columns
dates$diff = round(difftime(as.POSIXct(dates$to), as.POSIXct(dates$from), units = "days"))
dates = data.frame(dates, probs[TOprobs,])
# Reduce down to a single observation per trapping event instead of per species
allspecs =
dates %>%
dplyr::select("to", "from", "diff", "Specimens.collected", "Latitudes", "Locations", "Longitudes", "Collection.protocols", "Attractants") %>%
group_by(Locations, Latitudes, Longitudes, from, to, diff, Collection.protocols, Attractants) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(Latitudes, Longitudes, from, to)
# Check frequencies of different interval lengths
freqs = table(allspecs$diff)
freqs
# Look at locations where these are happening
unique(dates$Locations) # All locations
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - nrow(TOprobs) - nrow(commaprobs) - nrow(three_TOprobs) - nrow(three_commaprobs)
nrow(probs) - nrow(TOprobs) - nrow(commaprobs) - nrow(three_TOprobs) #- nrow(three_commaprobs)
nrow(probs)
nrow(TOprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs),]
probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs)]
four_commaprobs = probs$Collection.date.range[-c(TOprobs, commaprobs, three_TOprobs, three_commaprobs)]
four_commaprobs = three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
remain = probs$Collection.date.range[-c(TOprobs, commaprobs, three_commaprobs, three_TOprobs, four_commaprobs)]
remain
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(four_commaprobs)
table(probs$Collection.protocols[four_commaprobs])
library(stringr)
fddata = read.csv("~/Documents/Hefty_Data/dates_weird.csv", stringsAsFactors = F)
fddata = fddata[which(fddata$Locations != "Riverside"),]
class(fddata$Collection.date.range)
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Polk (Iowa (United States))")])
table(fddata$Collection.protocols[which(fddata$Locations == "Woodbury")])
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[four_commaprobs])
library(stats)
library(MARSS)
library(forecast)
library(datasets)
library(devtools)
library(tseries)
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
# Create ts object from the chosen species data: classical decomposition model
# Find the start of this dataset: c(month number of start, year of start)
start = c(d$Year[1], as.numeric(format(as.Date(d$date_dm[1], format="%Y-%m-%d"),"%m")))
# and the end:
#end = c(as.numeric(format(as.Date(d$date_dm[nrow(d)], format="%Y-%m-%d"),"%m")), d$Year[nrow(d)])
moz = ts(data = d$Culex.erraticus, frequency = 12, start = start)
# Now plot
plot.ts(moz) # violates stationarity due to 1. seasonality and 2. possibly increasing? could also be change in variance
# also make temp time series
temp = ts(data = d$temp_mean, frequency = 12, start = start)
# Merge our ts:
both = ts.union(moz, temp)
# Plot
plot(both, main = "Temp and abundance", yax.flip = T)
# for even numbered values of k, use only 1/2 weight for 2 most extreme values
ma_weights = c(1/2, rep(1, 11), 1/2)/12
# Estimate the trend (m):
moz_trend = filter(moz, filter = ma_weights, method = "convo", sides = 2)
# Plot the trend
plot.ts(moz_trend)
## Find seasonal effects + random error by subtraction
moz_seas = moz - moz_trend
plot.ts(moz_seas, ylab = 'seasonal effect + errors', xlab = 'month')
### Obtain average seasonal effect (per year): ###
# length of ts
ll = length(moz_seas)
## frequency (12 months)
ff = frequency(moz_seas)
## number of periods:
periods = ll%/%ff
# index of cumulative month
index = seq(1, ll, by = ff) - 1
# get mean by month
mm = numeric(ff)
for(i in 1:ff){
mm[i] = mean(moz_seas[index + i], na.rm = T)
}
mm
# Plot monthly seasonal effect
plot.ts(mm, ylab = 'Monthly Seasonal Effect', xlab = 'Month')
# Create time series of seasonal effects:
moz_ts_seas = ts(rep(mm, periods + 1)[seq(ll)], start = start(moz_seas), frequency = ff)
plot(moz_ts_seas)
# Complete the model by obtaining the random errors via subtraction
err = moz - moz_trend - moz_ts_seas
# Now we have 3 components: trend, seasonality, error: plot with data:
plot(cbind(moz, moz_trend, moz_ts_seas, err), main = "", yax.flip = T)
########## Use decompose() to do all of this all at once! WOW! ##################
moz_decomp = decompose(moz)
# figure out how many times you should difference:
ndiff(moz)
# figure out how many times you should difference:
ndiffs(moz)
##### Assess AutoCorrelation#######
## Use ACF and PACF to find q and p, respecitively for AR-I-MA model
acf(moz, lag.max = 36, na.action = na.pass) # na.pass deals with na values by interpolated them
# PACF- finds correlation of residuals with next lag value
# Sharp decrease in PACF means lags close to 0 do good job of predicting present
pacf(moz, lag.max = 36, na.action = na.pass)
######### Cross Correlation Between Time Series ##############
# plot data of interest
plot(cbind(temp, moz), yax.flip = T)
# Find cross correlation:
ccf(temp, moz, ylab = "Cross Correlation", na.action = na.pass)
# I want to see this x axis converted to month:
cc = ccf(temp, moz, ylab = "Cross Correlation", na.action = na.pass)
cc.x = cc$lag
cc.x.months = cc$lag*12
cc.y = cc$acf
nn = cc$n.used
plot(cc.y~cc.x.months, type = 'h', xlab = 'lag', ylab = 'ACF')
abline(h = -1/nn + c(-2, 2)/sqrt(nn), lty = "dashed", col = "blue")
abline(h = 0)
fit
### Use auto.arima() for seasonal ts
# subtract 1 year off moz data for testing forecastin
trainmoz = window(moz, start, c(2016, 12))
par(mfrow = c(3,1))
ndiffs(trainmoz)
plot(diff(trainmoz))
acf(diff(trainmoz))
pacf(diff(trainmoz))
testmoz = window(moz, c(2017, 1), c(2017, 12))
fit = auto.arima(trainmoz)
fit
dev.off()
plot(forecast(fit, h = 12))
points(testmoz)
fit$model
fit$arma
fit
ndiffs(temp)
fit2 = auto.arima(temp)
fit2
fit2$arma
fit3 = auto.arima(d$precip_days)
fit3
fit3$arma
fit5 = auto.arima(d$Specimens.collected)
fit5
fit5 = auto.arima(d$Aedes.aegypti)
fit5
fit5$arma
fit5 = auto.arima(d$Aedes.atlanticus)
fit5
fit5 = auto.arima(d$precip_mean)
fit5
fit5 = auto.arima(d$Culex.erraticus)
fit5
fit5 = auto.arima(ts(d$Culex.erraticus))
fit5
?ts
fit5 = auto.arima(ts(d$Culex.erraticus, frequency = 12))
fit5
fit5 = auto.arima(moz)
fit5
fit5$arma
auto.arima(ts(d$precip_mean, frequency = 12))
fit6 = auto.arima(d$precip_mean)
fit6
fit6$arma
# extract seasonal sarima model: format of auto.arima()$arma = p, q, P, Q, m, d, D
non_seas_comp = fit$arma[c(1, 2, 6)]
fit
seas_comp = fit$arma[c(3,4,7)]
freq = fit$arma[5]
plot(log(moz))
plot(log(d$Culex.erraticus))
plot(log(d$Culex.erraticus + 1))
density(log(d$Culex.erraticus + 1))
density(log(d$Culex.erraticus + 1), na.rm = T)
plot(density(log(d$Culex.erraticus + 1), na.rm = T))
plot(forecast(fit, h = 12))
plot(density(d$Culex.erraticus))
plot(density(d$Culex.erraticus), na.rm = T)
plot(density(d$Culex.erraticus, na.rm = T))
hist(d$Culex.erraticus)
hist(d$Culex.erraticus, breaks = 20)
hist(log(d$Culex.erraticus +1), breaks = 20)
hist(log(d$Culex.erraticus), breaks = 20)
log(0)
plot(ts(log(d$Culex.erraticus)))
plot(diff(ts(log(d$Culex.erraticus))))
fit = auto.arima(ts(log(d$Culex.erraticus)), frequency = 12)
ccf(temp, ts(log(d$Culex.erraticus)))
ccf(temp, ts(log(d$Culex.erraticus), frequency = 12))
?ts
test = ts(d$Culex.erraticus, start = start, frequency = 12)
test = ts(log(d$Culex.erraticus), start = start, frequency = 12)
plot.ts(test)
decompose(test)
plot(decompose(test))
?decompose
decompose(diff(test))
ccf(temp, test)
ccf(temp, test, na.action = na.pass)
plot(test)
plot(ts(log(d$Culex.erraticus + 1)))
plot(ts(d$Culex.erraticus))
hist(d$Culex.erraticus)
hist(d$Culex.erraticus, breaks = 20)
hist(log(d$Culex.erraticus), breaks = 20)
hist(log(d$Culex.erraticus + 1), breaks = 20)
plot(log(d$Culex.erraticus~d$temp_mean))
plot(log(d$Culex.erraticus)~d$temp_mean)
log(0)
log(0.000001)
exp(2.5)
length(d$erraticus == 0)/nrow(!is.na(d$erraticus))
length(d$erraticus == 0)
length(d$Culex.erraticus == 0)
length(d$Culex.erraticus == 0)/nrow(!is.na(d$Culex.erraticus))
length(d$Culex.erraticus == 0)/nrow(!is.na(d$Culex.erraticus))
nrow(!is.na(d$Culex.erraticus))
length(!is.na(d$Culex.erraticus))
!is.na(d$Culex.erraticus)
sum(!is.na(d$Culex.erraticus))
sum(d$Culex.erraticus == 0)
View(d$Culex.erraticus[which(d$Culex.erraticus == 0)])
?decompose
boxplot(d$Culex.erraticus)
?boxplot
boxplot(d$Culex.erraticus~d$Year)
points(testmoz)
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
moz1 = d$Culex.erraticus + 1
templ1 = d$temp_mean[1:(nrow(d)-1)]
test = c(1, 2, 3, 4, 5)
test[2:]
test[-1]
library(mgcv)
# Try simple GAM
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
# Create vector of abundance + 1
moz1 = d$Culex.erraticus + 1
templ1 = d$temp_mean[1:(nrow(d)-1)]
precipl1 = d$precip_days[1:(nrow(d)-1)]
mozl1 = moz1[1:(nrow(d)-1)]
fit = gam(moz1[-1]~ s(templ1) + s(precipl1) + s(log(mozl1)))
fit
summary(fit)
fit$aic
fit2 = gam(moz1[-1]~ s(templ1) + s(precipl1))
summary(fit2)
fit2$aic
fit = gam(moz1[-1]~ s(templ1) + s(precipl1) + s(log(mozl1)), family = Gamma(link = 'log'))
summary(fit)
fit$aic
fit2 = gam(moz1[-1]~ s(templ1) + s(precipl1), family = Gamma(link = 'log'))
summary(fit2)
fit2$aic
choose(3,2)
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F)
barplot(d$Best_TempLag)
barplot(table(d$Best_TempLag))
barplot(table(d$Best_TempLag), labels(cex = 0.8))
barplot(table(d$Best_TempLag), cex.names = 0.8)
barplot(table(d$Best_TempLag), cex.names = 0.5)
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.9)
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
len(d$Best_TempLag[40])
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_TempLag)
barplot(table(d$Best_PrecipMeanLag), cex.names = 0.7)
View(d)
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_TempLag)
barplot(table(d$Best_PrecipMeanLag), cex.names = 0.7)
table(d$Best_TempLag)
setwd("~/Documents/CMEEThesis/Code/")
# Read in abundace dataset
fddata = read.csv("../Data/fdcounts_datescleaned.csv", header = T, stringsAsFactors = F)
# Create vector of locations
counties = unique(fddata$Locations)
p = 1
vector_abun = fddata[fddata$Locations == counties[p],]
library(tidyverse)
library(lubridate)
library(readr)
library(sf)
library(rgdal)
library(maps)
library(ggplot2)
library(sp)
project_crs <- "+proj=longlat +WGS84 (EPSG: 4326) +init=epsg:4326 +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"
print(paste("Now starting climate extraction for", as.character(vector_abun$Location[1])))
# Adjust column names to match Matt's style
colnames(vector_abun) = gsub("\\.", " ", colnames(vector_abun))
# Extract the year from each date
vector_abun$Year <- format(as.Date(vector_abun$`Collection date range`), "%Y")
counties
p = 7
vector_abun = fddata[fddata$Locations == counties[p],]
print(paste("Now starting climate extraction for", as.character(vector_abun$Location[1])))
# Adjust column names to match Matt's style
colnames(vector_abun) = gsub("\\.", " ", colnames(vector_abun))
# Extract the year from each date
vector_abun$Year <- format(as.Date(vector_abun$`Collection date range`), "%Y")
# Clear the workspace
rm(list=setdiff(ls(), c("vector_abun", "project_crs", "p", "counties", "fddata")))
###################################################################################################################
#EXTRACTION TMAX - Note that the CPC climate data spans from 1979 to 2019,remove observations outside of this range
###################################################################################################################
#Load packages
library(RNetCDF)
library(raster)
library(rgdal)
library(sf)
library(sp)
project_crs <- "+proj=longlat +WGS84 (EPSG: 4326) +init=epsg:4326 +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"
# create a vector of years in the data set (we use this to select tmax files)
years <- dplyr::distinct(vector_abun, Year)
years <- as.vector(years$Year)
# get unique trap locations and create as sf object
traps <- dplyr::distinct(vector_abun, Latitudes, Longitudes)
traps = st_as_sf(traps, coords = c("Longitudes", "Latitudes"), crs = project_crs, remove = FALSE)
# Set the working directory to point to the tmax.nc files
setwd("../Data/tmax")
# create start message
print (paste(Sys.time(),"start"))
# count the number of rows in the traps dataframe so we can create an empty df
df_count <- NROW(traps)
# Create a new dataframe with the same number of rows as there are traps
tmaxAll <- as.data.frame(matrix(0, ncol = 0, nrow = df_count))
# Copy the geometry to the dataframe
tmaxAll$Latitudes <- traps$Latitudes
tmaxAll$Longitudes <- traps$Longitudes
# loop through each year in the years vector and look for matching file names in the Wd
for (y in 1:length(years))
{
# create file pattern
pat <-paste("tmax.",years[y], sep="")
year <- years[y]
#list the files matching the pattern
listfiles <- list.files(pattern = pat)
# create a loop that runs through every matching file in the wb
for(f in listfiles)
{
# create a raster brick, each file in the brick will represent a day of temperature sampling
tmax = brick(f) ## this is where tmp data folder is being filled up
# rotate the raster brick to convert to conventional -180 to 180 longitudes
tmax <- rotate(tmax) # took 1 gig
# replace -999 values with NA
tmax <- reclassify(tmax, cbind(-999, NA))
# create a day variable from the brick from each layer, representing each day in the year
days <- nlayers(tmax)
# ensure the Coordinate Reference System in the temp data and trap sites are matching
shp = st_transform(traps, crs(tmax))
#Extract data from each layer in the brick
for (i in 1:nlayers(tmax))
{
print (paste(Sys.time(),"extracting-", "dateset:", f, ",year",year, ",Day:",i))
#print(c(i, f))
tmaxEx <- raster::extract(tmax[[i]],
as_Spatial(shp),
method = 'bilinear',
fun = mean,
na.rm = T)
# create a dataframe out of the extracted data
tmaxexdf <- as.data.frame(tmaxEx)
# name the column name by year
colnames(tmaxexdf) <- paste(year, i, sep = "-")
# bind newly created dataframe to the tmaxall, each loop in the cycle will create a new column
tmaxAll <- cbind(tmaxAll,tmaxexdf)
}
# Find raster files in tmp directory
files_to_remove = list.files("/tmp/", pattern = "\\.gri", recursive = T, full.names = T)
files_to_remove = c(files_to_remove,
list.files("/tmp/", pattern = "\\.grd", recursive = T, full.names = T))
# remove these unneeded raster files
file.remove(files_to_remove)
}}
# Read in abundace dataset
fddata = read.csv("../Data/fdcounts_datescleaned.csv", header = T, stringsAsFactors = F)
setwd("~/Documents/CMEEThesis/Code/")
# Read in abundace dataset
fddata = read.csv("../Data/fdcounts_datescleaned.csv", header = T, stringsAsFactors = F)
# Create vector of locations
counties = unique(fddata$Locations)
counties
counties = counties[5:7]
pwd
getwd()
?list.files
## Begin loop to aggregate files
files = list.files(path = "../Data/Extracted_Data/", pattern = "_TS.csv")
files
i
i = 0
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
## Begin loop to aggregate files
files = list.files(path = "../Data/Extracted_Data/", pattern = "_TS.csv")
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
sub(".csv", "", files[i])
files[i]
i = 1
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
paste0("../Data/Extracted_Data/", files[i])
mapped = read.csv(paste0("../Data/Extracted_Data/", files[i]), header = T, stringsAsFactors = F)
source("aggregation.R")
source("aggregation.R")
write.csv(weekly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_monthly.csv"), row.names = F)
?dir.create
paste0("../Results/AggByLocation/",locale)
paste0(directory, "/aallspecs_weekly.jpeg")
directory = paste0("../Results/AggByLocation/", locale)
paste0(directory, "/aallspecs_weekly.jpeg")
j = 1
paste0(directory, name, "_weekly.jpeg")
name = species[j]
paste0(directory, name, "_weekly.jpeg")
