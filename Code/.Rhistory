#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(four_commaprobs)
table(probs$Collection.protocols[four_commaprobs])
library(stringr)
fddata = read.csv("~/Documents/Hefty_Data/dates_weird.csv", stringsAsFactors = F)
fddata = fddata[which(fddata$Locations != "Riverside"),]
class(fddata$Collection.date.range)
table(fddata$Collection.protocols[which(fddata$Locations == "Story")])
table(fddata$Collection.protocols[which(fddata$Locations == "Polk (Iowa (United States))")])
table(fddata$Collection.protocols[which(fddata$Locations == "Woodbury")])
# find the rows that are formatted correctly
indexes = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d$", fddata$Collection.date.range)
probs = fddata[-indexes,]
uprobs = unique(probs$Collection.date.range)
uprobs
# In some datasets, all issues are being caused by dates in the form:
# dddd-dd-dd TO dddd-dd-dd
#Check What's Going On With These:
TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_TOprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d TO \\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
three_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) - length(three_TOprobs) - length(three_commaprobs)
four_commaprobs = grep("^\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d,\\d\\d\\d\\d-\\d\\d-\\d\\d$", probs$Collection.date.range)
nrow(probs) - length(TOprobs) - length(commaprobs) -
length(three_TOprobs) - length(three_commaprobs) - length(four_commaprobs)
# Check trap type of each format
table(probs$Collection.protocols[TOprobs])
table(probs$Collection.protocols[commaprobs])
table(probs$Collection.protocols[three_TOprobs])
table(probs$Collection.protocols[three_commaprobs])
table(probs$Collection.protocols[four_commaprobs])
library(stats)
library(MARSS)
library(forecast)
library(datasets)
library(devtools)
library(tseries)
# load data
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
# Create ts object from the chosen species data: classical decomposition model
# Find the start of this dataset: c(month number of start, year of start)
start = c(d$Year[1], as.numeric(format(as.Date(d$date_dm[1], format="%Y-%m-%d"),"%m")))
# and the end:
#end = c(as.numeric(format(as.Date(d$date_dm[nrow(d)], format="%Y-%m-%d"),"%m")), d$Year[nrow(d)])
moz = ts(data = d$Culex.erraticus, frequency = 12, start = start)
# Now plot
plot.ts(moz) # violates stationarity due to 1. seasonality and 2. possibly increasing? could also be change in variance
# also make temp time series
temp = ts(data = d$temp_mean, frequency = 12, start = start)
# Merge our ts:
both = ts.union(moz, temp)
# Plot
plot(both, main = "Temp and abundance", yax.flip = T)
# for even numbered values of k, use only 1/2 weight for 2 most extreme values
ma_weights = c(1/2, rep(1, 11), 1/2)/12
# Estimate the trend (m):
moz_trend = filter(moz, filter = ma_weights, method = "convo", sides = 2)
# Plot the trend
plot.ts(moz_trend)
## Find seasonal effects + random error by subtraction
moz_seas = moz - moz_trend
plot.ts(moz_seas, ylab = 'seasonal effect + errors', xlab = 'month')
### Obtain average seasonal effect (per year): ###
# length of ts
ll = length(moz_seas)
## frequency (12 months)
ff = frequency(moz_seas)
## number of periods:
periods = ll%/%ff
# index of cumulative month
index = seq(1, ll, by = ff) - 1
# get mean by month
mm = numeric(ff)
for(i in 1:ff){
mm[i] = mean(moz_seas[index + i], na.rm = T)
}
mm
# Plot monthly seasonal effect
plot.ts(mm, ylab = 'Monthly Seasonal Effect', xlab = 'Month')
# Create time series of seasonal effects:
moz_ts_seas = ts(rep(mm, periods + 1)[seq(ll)], start = start(moz_seas), frequency = ff)
plot(moz_ts_seas)
# Complete the model by obtaining the random errors via subtraction
err = moz - moz_trend - moz_ts_seas
# Now we have 3 components: trend, seasonality, error: plot with data:
plot(cbind(moz, moz_trend, moz_ts_seas, err), main = "", yax.flip = T)
########## Use decompose() to do all of this all at once! WOW! ##################
moz_decomp = decompose(moz)
# figure out how many times you should difference:
ndiff(moz)
# figure out how many times you should difference:
ndiffs(moz)
##### Assess AutoCorrelation#######
## Use ACF and PACF to find q and p, respecitively for AR-I-MA model
acf(moz, lag.max = 36, na.action = na.pass) # na.pass deals with na values by interpolated them
# PACF- finds correlation of residuals with next lag value
# Sharp decrease in PACF means lags close to 0 do good job of predicting present
pacf(moz, lag.max = 36, na.action = na.pass)
######### Cross Correlation Between Time Series ##############
# plot data of interest
plot(cbind(temp, moz), yax.flip = T)
# Find cross correlation:
ccf(temp, moz, ylab = "Cross Correlation", na.action = na.pass)
# I want to see this x axis converted to month:
cc = ccf(temp, moz, ylab = "Cross Correlation", na.action = na.pass)
cc.x = cc$lag
cc.x.months = cc$lag*12
cc.y = cc$acf
nn = cc$n.used
plot(cc.y~cc.x.months, type = 'h', xlab = 'lag', ylab = 'ACF')
abline(h = -1/nn + c(-2, 2)/sqrt(nn), lty = "dashed", col = "blue")
abline(h = 0)
fit
### Use auto.arima() for seasonal ts
# subtract 1 year off moz data for testing forecastin
trainmoz = window(moz, start, c(2016, 12))
par(mfrow = c(3,1))
ndiffs(trainmoz)
plot(diff(trainmoz))
acf(diff(trainmoz))
pacf(diff(trainmoz))
testmoz = window(moz, c(2017, 1), c(2017, 12))
fit = auto.arima(trainmoz)
fit
dev.off()
plot(forecast(fit, h = 12))
points(testmoz)
fit$model
fit$arma
fit
ndiffs(temp)
fit2 = auto.arima(temp)
fit2
fit2$arma
fit3 = auto.arima(d$precip_days)
fit3
fit3$arma
fit5 = auto.arima(d$Specimens.collected)
fit5
fit5 = auto.arima(d$Aedes.aegypti)
fit5
fit5$arma
fit5 = auto.arima(d$Aedes.atlanticus)
fit5
fit5 = auto.arima(d$precip_mean)
fit5
fit5 = auto.arima(d$Culex.erraticus)
fit5
fit5 = auto.arima(ts(d$Culex.erraticus))
fit5
?ts
fit5 = auto.arima(ts(d$Culex.erraticus, frequency = 12))
fit5
fit5 = auto.arima(moz)
fit5
fit5$arma
auto.arima(ts(d$precip_mean, frequency = 12))
fit6 = auto.arima(d$precip_mean)
fit6
fit6$arma
# extract seasonal sarima model: format of auto.arima()$arma = p, q, P, Q, m, d, D
non_seas_comp = fit$arma[c(1, 2, 6)]
fit
seas_comp = fit$arma[c(3,4,7)]
freq = fit$arma[5]
plot(log(moz))
plot(log(d$Culex.erraticus))
plot(log(d$Culex.erraticus + 1))
density(log(d$Culex.erraticus + 1))
density(log(d$Culex.erraticus + 1), na.rm = T)
plot(density(log(d$Culex.erraticus + 1), na.rm = T))
plot(forecast(fit, h = 12))
plot(density(d$Culex.erraticus))
plot(density(d$Culex.erraticus), na.rm = T)
plot(density(d$Culex.erraticus, na.rm = T))
hist(d$Culex.erraticus)
hist(d$Culex.erraticus, breaks = 20)
hist(log(d$Culex.erraticus +1), breaks = 20)
hist(log(d$Culex.erraticus), breaks = 20)
log(0)
plot(ts(log(d$Culex.erraticus)))
plot(diff(ts(log(d$Culex.erraticus))))
fit = auto.arima(ts(log(d$Culex.erraticus)), frequency = 12)
ccf(temp, ts(log(d$Culex.erraticus)))
ccf(temp, ts(log(d$Culex.erraticus), frequency = 12))
?ts
test = ts(d$Culex.erraticus, start = start, frequency = 12)
test = ts(log(d$Culex.erraticus), start = start, frequency = 12)
plot.ts(test)
decompose(test)
plot(decompose(test))
?decompose
decompose(diff(test))
ccf(temp, test)
ccf(temp, test, na.action = na.pass)
plot(test)
plot(ts(log(d$Culex.erraticus + 1)))
plot(ts(d$Culex.erraticus))
hist(d$Culex.erraticus)
hist(d$Culex.erraticus, breaks = 20)
hist(log(d$Culex.erraticus), breaks = 20)
hist(log(d$Culex.erraticus + 1), breaks = 20)
plot(log(d$Culex.erraticus~d$temp_mean))
plot(log(d$Culex.erraticus)~d$temp_mean)
log(0)
log(0.000001)
exp(2.5)
length(d$erraticus == 0)/nrow(!is.na(d$erraticus))
length(d$erraticus == 0)
length(d$Culex.erraticus == 0)
length(d$Culex.erraticus == 0)/nrow(!is.na(d$Culex.erraticus))
length(d$Culex.erraticus == 0)/nrow(!is.na(d$Culex.erraticus))
nrow(!is.na(d$Culex.erraticus))
length(!is.na(d$Culex.erraticus))
!is.na(d$Culex.erraticus)
sum(!is.na(d$Culex.erraticus))
sum(d$Culex.erraticus == 0)
View(d$Culex.erraticus[which(d$Culex.erraticus == 0)])
?decompose
boxplot(d$Culex.erraticus)
?boxplot
boxplot(d$Culex.erraticus~d$Year)
points(testmoz)
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
moz1 = d$Culex.erraticus + 1
templ1 = d$temp_mean[1:(nrow(d)-1)]
test = c(1, 2, 3, 4, 5)
test[2:]
test[-1]
library(mgcv)
# Try simple GAM
d = read.csv("~/Documents/Hefty_Data/Extracted_Data/Aggregated/Orange _monthly.csv", header = T, stringsAsFactors = F)
# Create vector of abundance + 1
moz1 = d$Culex.erraticus + 1
templ1 = d$temp_mean[1:(nrow(d)-1)]
precipl1 = d$precip_days[1:(nrow(d)-1)]
mozl1 = moz1[1:(nrow(d)-1)]
fit = gam(moz1[-1]~ s(templ1) + s(precipl1) + s(log(mozl1)))
fit
summary(fit)
fit$aic
fit2 = gam(moz1[-1]~ s(templ1) + s(precipl1))
summary(fit2)
fit2$aic
fit = gam(moz1[-1]~ s(templ1) + s(precipl1) + s(log(mozl1)), family = Gamma(link = 'log'))
summary(fit)
fit$aic
fit2 = gam(moz1[-1]~ s(templ1) + s(precipl1), family = Gamma(link = 'log'))
summary(fit2)
fit2$aic
choose(3,2)
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F)
barplot(d$Best_TempLag)
barplot(table(d$Best_TempLag))
barplot(table(d$Best_TempLag), labels(cex = 0.8))
barplot(table(d$Best_TempLag), cex.names = 0.8)
barplot(table(d$Best_TempLag), cex.names = 0.5)
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.9)
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
len(d$Best_TempLag[40])
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_TempLag)
barplot(table(d$Best_PrecipMeanLag), cex.names = 0.7)
View(d)
d = read.csv('~/Documents/CMEEThesis/Results/Lee_biweekly_output2.csv', header = T, stringsAsFactors = F, na.strings=c("","NA"))
barplot(table(d$Best_TempLag), cex.names = 0.9)
table(d$Best_TempLag)
class(d$Best_TempLag[40])
barplot(table(d$Best_PrecipDaysLag), cex.names = 0.7)
table(d$Best_TempLag)
barplot(table(d$Best_PrecipMeanLag), cex.names = 0.7)
table(d$Best_TempLag)
## Begin loop to aggregate files
files = list.files(path = "../Data/Extracted_Data/", pattern = "_TS.csv")
setwd("~/Documents/CMEEThesis/Code/")
## Begin loop to aggregate files
files = list.files(path = "../Data/Extracted_Data/", pattern = "_TS.csv")
i = 1
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
mapped = read.csv(paste0("../Data/Extracted_Data/", files[i]), header = T, stringsAsFactors = F)
source("aggregation.R")
write.csv(weekly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_monthly.csv"), row.names = F)
# Create a directory to store plots
dir.create(paste0("../Results/AggByLocation/",locale))
directory = paste0("../Results/AggByLocation/", locale)
# Create a directory to store plots
dir.create(paste0("../Results/AggByLocation/",locale))
directory = paste0("../Results/AggByLocation/", locale)
# Weekly
jpeg(paste0(directory, "/aallspecs_weekly.jpeg"))
print(ggplot(weekly, aes(x=date_dw, y = `Specimens.collected`)) +
geom_line(col = "darkgreen") + xlab("") +
scale_x_date(date_labels = "%Y") +
ggtitle(paste0(locale, ", all species, Weekly Mean")) +
theme_bw() + labs(y = "Mosquitoes Collected",
x = "Time"))
dev.off()
paste0("../Results/AggByLocation/", locale)
paste0(directory, "/", name, "_weekly.jpeg")
j = 1
name = species[j]
paste0(directory, "/", name, "_weekly.jpeg")
files
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
i = 3
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
mapped = read.csv(paste0("../Data/Extracted_Data/", files[i]), header = T, stringsAsFactors = F)
source("aggregation.R")
locale
# Read in abundace dataset
fddata = read.csv("../Data/fdcounts_datescleaned.csv", header = T, stringsAsFactors = F)
# Create vector of locations
counties = unique(fddata$Locations)
counties
p = 5
vector_abun = fddata[fddata$Locations == counties[p],]
source("climate_extraction.R")
locale
filename = paste("../Data/Extracted_Data/", locale,
"_clim_TS.csv", sep = "")
write.csv(vector_abun_clim_time_series, file = filename, row.names = F)
setwd('~/Documents/CMEEThesis/Code/')
## Begin loop to aggregate files
files = list.files(path = "../Data/Extracted_Data/", pattern = "_TS.csv")
i = 3
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
mapped = read.csv(paste0("../Data/Extracted_Data/", files[i]), header = T, stringsAsFactors = F)
library(tidyverse)
# Sort the data
mapped = arrange(mapped, date, Latitudes, Longitudes)
# Extract the location name, without special characters
locale = as.character(na.omit(mapped$Locations)[1])
locale = sub(" \\(.*", "", locale) # remove special characters
locale = sub("\\s", "_", locale) # Convert any remaining spaces to underscore
# Convert date from character to date format
mapped$date = as.Date(mapped$date)
# Create a new data frame which aggregates all species to a single count per trap
allspecs =
mapped %>%
dplyr::select("date", "Specimens.collected", "Latitudes", "Longitudes", "Year", "precip", "max_temp") %>%
group_by(Latitudes, Longitudes, date, Year, precip, max_temp) %>%
summarise(Specimens.collected = mean(Specimens.collected)) %>%
arrange(date, Latitudes, Longitudes)
# Find where at least some species had specimens collected
allspecs$real = allspecs$Specimens.collected > 0 # where SC = NA, this will = NA so:
allspecs$real[which(is.na(allspecs$real))] = FALSE
# Keep only real and columns needed for join
to_join = allspecs[,-c(4:7)]
# Join this to mapped data frame
mapped = left_join(mapped, to_join, by = c("date", "Latitudes", "Longitudes"))
# First set all NA values to zero
mapped$Specimens.collected[is.na(mapped$Specimens.collected)] = 0
# But on days where no samples were taken, set all zeroes to NA: likely to have zero species collected and NA for year
mapped$Specimens.collected = replace(mapped$Specimens.collected, mapped$real == FALSE & is.na(mapped$Year), NA)
# Convert all species name spaces to periods:
mapped$Species = gsub(" ", ".", mapped$Species)
# Also convert any weird symbols to periods:
mapped$Species = gsub("\\W", ".", mapped$Species)
# Create an alphebetical list of unique species names
species = sort(unique(mapped$Species[which(!is.na(mapped$Species))]))
# give each species its own column
for(j in 1:length(species)){
# Get all the rows with counts of those species
spec_counts = mapped[mapped$Species == species[j],]
# Summarise the mean samples of this species from each day
spec_counts =
spec_counts %>%
group_by(Latitudes, Longitudes, date) %>%
dplyr::select("date", "Latitudes", "Longitudes", "Specimens.collected") %>%
summarise(test = mean(Specimens.collected))
# Change column names so that count column identifies the species
names(spec_counts)[names(spec_counts) == 'test'] = species[j]
# join to the daily data frame by date
allspecs = left_join(allspecs, spec_counts, by = c("date", "Latitudes", "Longitudes"))
}
# find columns of species within each morphological group
# culex pipiens
cp_spec = c("Culex.salinarius", "Culex.quinquefasciatus", "Culex.pipiens", "Culex.restuans", "Culex.australicus")
cp_morphs = grep(paste(cp_spec, collapse = "|"), colnames(allspecs))
#Aedes atlanticus-tormentor
aat_spec = c("Aedes.atlanticus", "Aedes.tormentor")
aat_morphs = grep(paste(aat_spec, collapse = "|"), colnames(allspecs))
# Anopheles messeae/daciae
amd_spec = c("Anopheles.messeae", "Anopheles.daciae")
amd_morphs = grep(paste(amd_spec, collapse = "|"), colnames(allspecs))
# Create a morphological group column that is the mean of these groups
allspecs$C.pipiens.morphological.group = rowMeans(allspecs[,cp_morphs], na.rm = T)
allspecs$A.atlanticus.tormentor.morphological.group = rowMeans(allspecs[,aat_morphs], na.rm = T)
allspecs$A.messeae.daciae.morphological.group = rowMeans(allspecs[,amd_morphs], na.rm = T)
allspecs[, -c(amd_morphs)]
test = allspecs[, -c(amd_morphs)]
test
test = allspecs[, -c(cp_morphs)]
test = allspecs[, -c(cp_morphs, amd_morphs, aat_morphs)]
length(amd_morphs) > 0
allspecs$A.messeae.daciae.morphological.group
sum(!is.nan(allspecs$A.messeae.daciae.morphological.group))
getwd()
## Begin loop to aggregate files
files = list.files(path = "../Data/Extracted_Data/", pattern = "_TS.csv")
for(i in 1:length(files)){
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
mapped = read.csv(paste0("../Data/Extracted_Data/", files[i]), header = T, stringsAsFactors = F)
source("aggregation.R")
write.csv(weekly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_weekly.csv"), row.names = F)
write.csv(biweekly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_biweekly.csv"), row.names = F)
write.csv(monthly, paste0("../Data/Extracted_Data/Aggregated/", locale, "_monthly.csv"), row.names = F)
### Plot ###
#setwd("../Results/AggByLocation/")
# Create a directory to store plots
dir.create(paste0("../Results/AggByLocation/",locale))
directory = paste0("../Results/AggByLocation/", locale)
## Create plots of all species aggregated at weekly, biweekly, and monthly time scales ##
# Weekly
jpeg(paste0(directory, "/aallspecs_weekly.jpeg"))
print(ggplot(weekly, aes(x=date_dw, y = `Specimens.collected`)) +
geom_line(col = "darkgreen") + xlab("") +
scale_x_date(date_labels = "%Y") +
ggtitle(paste0(locale, ", all species, Weekly Mean")) +
theme_bw() + labs(y = "Mosquitoes Collected",
x = "Time"))
dev.off()
## Biweekly
# Create manual tick marks and axis labels
brks = seq(0, 27 * length(yrs), by = 27)
next_year = as.character(as.numeric(yrs[length(yrs)]) + 1)
jpeg(paste0(directory, "/aallspecs_biweekly.jpeg"))
print(ggplot(biweekly, aes(x=ids, y = `Specimens.collected`)) +
geom_line(col = "blue") + xlab("") +
scale_x_continuous("Time", breaks = brks, labels = c(yrs, next_year)) +
ggtitle(paste(locale,"all species, Biweekly Mean")) +
theme_bw() + labs(y = "Mosquitoes Collected"))
dev.off()
# Monthly
jpeg(paste0(directory, "/aallspecs_monthly.jpeg"))
print(ggplot(monthly, aes(x=date_dm, y = `Specimens.collected`)) +
geom_line(col = "purple") + xlab("") +
scale_x_date(date_labels = "%Y") +
ggtitle(paste0(locale, " all species, Monthly Mean")) +
theme_bw() + labs(y = "Mosquitoes Collected",
x = "Time"))
dev.off()
# Loop through each species, making 3 plots for each
for(j in 1:length(species)){
name = species[j]
# Weekly
jpeg(paste0(directory, "/", name, "_weekly.jpeg"))
print(ggplot(weekly, aes_string(x= "date_dw", y = name)) +
geom_line(col = "darkgreen") + xlab("") +
scale_x_date(date_labels = "%Y") +
ggtitle(paste(locale, name, "Weekly")) +
theme_bw() + labs(y = "Mosquitoes Collected",
x = "Time"))
dev.off()
# Bi-weekly
jpeg(paste0(directory, "/", name, "_biweekly.jpeg"))
print(ggplot(biweekly, aes_string(x= "ids", y = name)) +
geom_line(col = "blue") + xlab("") +
scale_x_continuous("Time", breaks = brks, labels = c(yrs, next_year)) +
ggtitle(paste(locale, name, "Biweekly")) +
theme_bw() + labs(y = "Mosquitoes Collected"))
dev.off()
# Monthly
jpeg(paste0(directory, "/", name, "_monthly.jpeg"))
print(ggplot(monthly, aes_string(x="date_dm", y = name)) +
geom_line(col = "purple") + xlab("") +
scale_x_date(date_labels = "%Y") +
ggtitle(paste(locale, name, "Monthly")) +
theme_bw() + labs(y = "Mosquitoes Collected",
x = "Time"))
dev.off()
}
rm(list=setdiff(ls(), c("i", "files", "continue")))
}
species
colnames(allspecs)
c(1, 2, 3) + 8
species[-(c(amd_morphs, aat_morphs, cp_morphs) - 8)]
files
i = 3
cat(paste("Now aggregating data for", sub(".csv", "", files[i]), "\n"))
mapped = read.csv(paste0("../Data/Extracted_Data/", files[i]), header = T, stringsAsFactors = F)
source("aggregation.R")
weekly$A.atlanticus.tormentor.morphological.group
weekly$Aedes.albopictus
